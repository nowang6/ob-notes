
# 机器学习

回归，分类，聚类
![[Pasted image 20231028193522.png]]


# 优化器

## Adam
- 自适应调整学习率：Adam 优化器可以根据历史梯度信息来自适应地调节学习率，使得在训练初期使用较大的学习率，能够快速收敛，在训练后期使用较小的学习率，能够更加准确地找到损失函数的最小值。
- 调整动量：Adam 优化器能够调整动量参数，以平衡上一次梯度和当前梯度对参数更新的影响，从而避免过早陷入局部极小值。
- 归一化处理：Adam 优化器对参数的更新进行了归一化处理，使得每个参数的更新都有一个相似的量级，从而提高训练效果。
- 防止过拟合：Adam 优化器结合了L2正则化的思想，在更新时对参数进行正则化，从而防止神经网络过度拟合训练数据。

## AdamW
权重衰减（Weight Decay）是一种正则化技术，用于防止深度学习模型的过拟合。

## LAMB
LAMB 优化器是 2019 年出现的一匹新秀，它将bert模型的预训练时间从3天压缩到了76分钟！ LAMB 出现的目的是加速预训练进程，这个优化器也成为 NLP 社区为泛机器学习领域做出的一大贡献。在使用 Adam 和 AdamW 等优化器时，一大问题在于 batch size 存在一定的隐式上限，一旦突破这个上限，梯度更新极端的取值会导致自适应学习率调整后极为困难的收敛，从而无法享受增加的 batch size 带来的提速增益。LAMB 优化器的作用便在于使模型在进行大批量数据训练时，能够维持梯度更新的精度。具体来说，LAMB 优化器支持自适应元素级更新（adaptive element-wise updating）和准确的逐层修正（layer-wise correction）




# 归一化函数

## SigMoid 二分类

它的导数，正态分布

$\frac{1} {1 + e^(-x) }$

```python
def sigmoid(x):  
    return 1/(1 + np.exp(-x))

x = np.linspace(-10, 10, 100)  
y = sigmoid(x)  
plt.plot(x, y)  
plt.xlabel("x")  
plt.ylabel("Sigmoid(X)")  
plt.show()
```


## SoftMax 多分类

## Batch Norm

## Layer Norm

![[Pasted image 20240723114933.png]]

### RMS Norm
RMS Norm是一般LayerNorm的一种变体，可以在梯度下降时令损失更加平滑，与layerNorm相比，RMS Norm的主要区别在于去掉了减去均值的部分(re-centering)，只保留方差部分(re-scaling)
![[Pasted image 20240723114954.png]]
少了Beta可训练参数


# 激活函数


## Relu

## SwiGLU
x*SigMoid

f(x) = x * sigmoid(βx)
- β 趋近于0 时，Swish 函数趋近于线性函数y = x
- 当β 趋近于无穷大时，Swish 函数趋近ReLU激活函数
- β 取值为1 时，Swish 函数是光滑且非单调。模型一般使用这个值




## Sigmoid/Logistic/逻辑函数
$$
σ(x) = \frac{1}{1 + e^{-x}}
$$

## tanh函数取值（-1，1）
, 多用于循环神经网络
```python
x = np.linspace(-10, 10, 100)
y = np.tanh(x)
plt.plot(x, y)
plt.show()
```


## softmax函数
多用于分类
``` python
def softmax(x):
	e_x = np.exp(x)
	return e_x / e_x.sum(axis=0)
	
x = np.array([1.0, 2.0, 3.0])
softmax_x = softmax(x)
print(softmax_x)

```


损失的种类
## MSE 均方差损失
计算实际值与预测值之差的平方的平均值，用于评估预测的准确性
## MAE 平均绝对损失
计算实际值与预测值之差的绝对值的平均值

## 对数损失 / 交叉熵损失
评估分类模型的概率预测的准确性，值越小表示模型性能越好。**一般用于分类问题**

## 交叉熵损失


# 距离
## 欧式距离
## 曼哈顿距离
## cos距离，内积距离
## KL距离
不具备对称性，A与B的距离不等于B与A的距离
物理意义：信息量差异的加权求和