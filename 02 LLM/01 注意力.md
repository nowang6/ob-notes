

## 原始形式：多头注意力 MHA

问题
- 训练过程：不会显著影响
- 推理过程：反复加载巨大的 KV Cache

## 多查询注意力MQA
不同的注意力头共享K和V, 减少KV Cache大小
ChatGLM


## 分组查询注意力GQA
MHA和MQA的折中，一般分为4组或8组
GQA
LLaMA

![[Pasted image 20240615144534.png]]

## FlashAttention




# 注意力架构

| 架构        | 设计者                                               | 特点                                     | 链接                                                                                                   |
| ----------- | ---------------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| Transformer | Google                                               | 最流行，几乎所有大模型都用它             | [OpenAI 的代码](https://github.com/openai/finetune-transformer-lm/blob/master/train.py)                |
| RWKV        | [PENG Bo](https://www.zhihu.com/people/bopengbopeng) | 可并行训练，推理性能极佳，适合在端侧使用 | [官网](https://www.rwkv.com/)、[RWKV 5 训练代码](https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v5) |
| Mamba       | CMU & Princeton University                           | 性能更佳，尤其适合长文本生成             | [GitHub](https://github.com/state-spaces/mamba)                                                        |


## Decode-Only / Cacsual Decoder-Only
有良好的扩展性和Zeo-Shot性能
CPT, LlaMA

## Prefix-Decoder / Non - Causal Decoded - Only
输入双向注意力，输出为单向自注意力
GLM


## Encode - Decode







# PE Position Encode位置编码
## 三角函数 / 绝对位置编码
外推能力有限
考虑了每个token的绝对位置，忽略了token之间的相对位置。与当前位置相邻的token往往在语义上更相关。

## 旋转位置编码 / 相对位置编码
通过线性插值等方法，无须重新预训练，就能扩展上下文长度。

使用欧拉公式构造旋转矩阵，分别将q和k旋转到空间中对应的位置，实现对计算结果添加位置信息，再计算qm@kn




# 上下文扩展

YaRN
RoPE
ALiBi位置编码具备良好的外推性
NTK