

# 向量的运算

## 加和减

![[Screen Shot 2023-08-27 at 4.11.18 PM.png|500]]

##  哈达玛积（Hadamard Product）
对应元素乘积， 结果是一个向量，在反向传播和梯度下降中使用
```
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = a.clone()
c = a * b
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```

##  点积（Dot Product）结果是矢量
对应元素乘积的和， 结果是一个标量
点积>=0
0, 正交
```python 
torch.dot(x, y) == torch.sum(x * y)
```

物理：做的功
```
（a,b).(c,d) = a*c + b*d
```
跟矩阵的乘法是相通的
```
(a,b)  .  (c,  =a*c+b*d
           d)     
```

![[Screen Shot 2023-08-27 at 4.14.44 PM.png]]


## 叉积（Cross Product）

仅适用于三维向量
![[Screen Shot 2023-08-27 at 4.16.44 PM.png]]

# 矩阵
## 矩阵乘向量torch.mv， 结果为向量
```python
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = torch.arange(4, dtype=torch.float32)

tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])

tensor([ 0.,  1.,  2.,  3.])

torch.mv(a, b)
tensor([ 14.,  38.,  62.,  86., 110.])

```

## 矩阵乘矩阵torch.mm， 结果为矩阵
```python
a = torch.arange(20, dtype=torch.float32).reshape(5, 4)
b = torch.arange(20, dtype=torch.float32).reshape(4, 5)
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])

tensor([[ 0.,  1.,  2.,  3.,  4.],
        [ 5.,  6.,  7.,  8.,  9.],
        [10., 11., 12., 13., 14.],
        [15., 16., 17., 18., 19.]])
torch.mm(a,b)
```

## 矩阵的秩 - 极大线性无关组
![[Pasted image 20230909181453.png]]

```python
import numpy as np

# 定义一个非方阵的矩阵
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [4, 5, 6]])

# 计算矩阵的秩
rank = np.linalg.matrix_rank(matrix)

print("矩阵的秩为:", rank)
```


## 矩阵的乘法

前行乘后列， 前面的第一行，乘以后面的第三列
![[Pasted image 20230909170157.png]]

向量乘矩阵

a1 * b
a2 * b
a3 * b


![[Pasted image 20230923165405.png]]


## 已知基底，求向量的坐标

![[Pasted image 20230909173346.png]]

# 范数

## 二范数和夹角

![[2023-08-271.png | 500]]
![[Screen Shot 2023-08-27 at 4.33.29 PM.png|300]]


```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

#计算向量夹角
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

cos_angle = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
angle = np.arccos(cos_angle)

#余弦相似度
import torch.nn.functional as F

vec1 = torch.FloatTensor([1, 2, 3, 4])
vec2 = torch.FloatTensor([5, 6, 7, 8])

cos_sim = F.cosine_similarity(vec1, vec2, dim=0)
```

## 计算2个点之间的距离
```python
a = torch.tensor([3.0, -4.0])
b = torch.tensor([1,1])
res = torch.norm(a-b)
```


## 标准正交基


# 资料
https://mp.weixin.qq.com/s/zOp7JvpmQDToCUHEcmokyw


# 其他

## 共现矩阵 / co-occurrence matrix



# 数
## 实数
有理数：可以用分号，小数表示
无理数：根号2， π，e等

## 虚数
i**2=-1

## 复数

形如a+bi（a、b均为实数）的数为复数，其中，a被称为实部，b被称为虚部，i为虚数单位。
i的平方等于-1

# 微积分
## 微分
函数在某一点的变化率/切线，某一点的导数

## 不定积分
求导的逆函数，用于求解原函数

## 定积分
从a到b, 跟x轴的面积
牛顿-莱布尼茨公式

$$
\int_a^b f(x) dx = F(b) - F(a)
$$


t表示时间，v表示速度，面积表示这段时间行驶的距离

![xx|500](20230827132608.png)

## 牛顿求根法
$$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
$$

求根号2，求解f(x)=x\*\*2 - 2， 在y=0点的根
猜测x<sub>0</sub>=2
x<sub>1</sub> = 2 - (2\*\*2-2)/2*2= 1.5

## 泰勒公式
将一个函数在某一点附近用无限项的多项式进行展开，展开的结果也是一个函数

$$
f(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \frac{f'''(a)}{3!}(x - a)^3 + \ldots
$$



## 微积分
x（向量）是叶子节点，z（向量）是中间节点，y（标量）是输出节点
Tensor的属性，requires_grad， grad微分值，grad_fn微分函数
- 当叶子节点的requires_grad为True时，信息流过该节点，所有中间节点的requires_grad都变为true
- 只要在输出节点调用backward(), Pytorch就会自动更新叶子节点的微分值，存储在叶子节点的grad属性里
- 只有叶子节点的grad属性能被更新
### 梯度-前向传播
```python
x = torch.ones(2)
x.requires_grad=True
z= 4*x
tensor([4., 4.], grad_fn=<MulBackward0>) # tensor是一个矢量
y=z.norm()
tensor(5.6569, grad_fn=<LinalgVectorNormBackward0>) # tensor是一个标量
```
### 梯度-反向传播
tensor做计算都会产生计算图，用于反向传播，计算梯度
```python
x.backward() # 报错grad can be implicitly created only for scalar outputs， 只能作用于标量
y.backward()
x.grad
tensor([2.8284, 2.8284])
z.grad
y.grad
```

计算函数y=x*x的梯度
```python
a = torch.arange(-3,4, dtype=torch.float32)
a.requires_grad = True
b = a * a
c = b.norm()
c.backward()

print(a.grad)
tensor([-3.8571, -1.1429, -0.1429,  0.0000,  0.1429,  1.1429,  3.8571]) # 在0点的梯度为0， 2边梯度逐渐变大
```
**backward() 只能作用于标量，如果是矢量，需要增加一个参数gradient, gradientde的形状跟输出节点的形状保持一致，而且元素均值为1**



# 概率论

## 条件概率
![[Pasted image 20230910215215.png]]


## 贝叶斯公式

![[Pasted image 20230910215138.png]]



## 高斯概率密度函数

![[Pasted image 20230910203403.png]]


## 条件概率和贝叶斯

A盒子：3黑，4白
B 盒子：1黑，8白

P(Black|BoxA):  已知巧克力来自A盒，请问黑色的概率
P(BoxA):  巧克力来自A盒的概率
P(Black and BoxA):  巧克力来自A盒，并且是黑色的概率
![[ml-1.png|500]]
已知来自A盒，黑色的概率？
![[ml-2.png|200]]

```
已知是黑色，来自A盒的概率？
P(BoxA | Black): 已知黑色，来自A盒的概率
P(Black):  黑色的概率, 全概率公式， 3/7*7/16 + 1/9 * 9/16
P(Black and BoxA)= P(Black | BoxA) * P(BoxA) = 3/7 * 7/16
```



![[Pasted image 20240628221659.png]]


![[ml-4.png|200]]






# 信息论

## 交叉熵CrossEntropy Loss
- KL散度 = 交叉熵 - 熵
- 交叉熵永远大于熵，因为KL散度永远大于等于零
- 最大似然估计跟交叉熵的最小化是对等的


## KL散度(相对熵)
衡量2个概率分布的差异，当2个概率分布完全相等时，KL散度为0


# 极大似然
似然函数
![[Pasted image 20240415062521.png]]
