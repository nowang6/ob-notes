# 评估指标

## BLEU 
Bilingual Evaluation Understudy
翻译评估, 衡量模型生成的文本与一组参考文本的相似度
例如：

　　　　原文：今天天气不错

　　　　机器译文：It is a nice day today

　　　　人工译文：Today is a nice day  

　　如果用1-gram匹配的话：

　　　![[Pasted image 20240727194320.png]]

　　可以看到机器译文一共6个词，有5个词语都命中的了参考译文，那么它1-gram的匹配度为 5/6


## ROUGE
Recall-Oriented Understudy for Gisting Evaluation）/ 对话
自动摘要评价指标， 也可以用于对话模型

## GLUE 
General Language Understanding Evaluation / 自然语言理解
9 个 NLU 的任务， 8 个分类任务（1个三分类，7 个二分类），一个回归任务(STS-B)



# 词嵌入Embedding
Leaderboard: https://huggingface.co/spaces/mteb/leaderboard
## MTEB: Massive Text Embedding Benchmark(海量⽂本嵌⼊基准)
https://huggingface.co/spaces/mteb/leaderboard

## C-MTEB


评价
正例数量 << 反例数量
![[Pasted image 20240218195752.png|600]]
![[Pasted image 20240218195852.png|600]]
## Accuracy 准确率 
not Enough

```python
>>> accuracy_metric = evaluate.load("accuracy") 
>>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1]) 
>>> print(results) {'accuracy': 1.0}
```
## Percision精准率/精确率/查准率
分母： TP
分子：TP+FP
正例预测对1个，反例预测错2个，精准率33%
```python
precision_metric = evaluate.load("precision")
results = precision_metric.compute(references=[1, 1, 0, 0, 0, 0, 0], predictions=[1, 0, 1, 1, 0, 0, 0])
print(results)
```
## Recall  召回率
分母： TP
分子：TP+FN
 一共2个正例，预测正确1个，召回率为50%
```python
recall_metric = evaluate.load('recall')
results = recall_metric.compute(references=[1, 1, 0, 0, 0, 0], predictions=[1, 0, 1, 1, 0, 0])
print(results)
```


# OpenComass

```
python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --hf-path /home/niwang/models/Meta-Llama-3-8B-Instruct -a lmdeploy
```